---
title: "Human Activity Recognition"
author: "Kieran Harding"
date: "03/03/2019"
output: 
    html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(caret)
library(randomForest)
```

# Summary

The Weight Lifting Exercises dataset (WLE) was prepared to test recognition
of how well a given exercise (Unilateral Dumbbell Biceps Curl) using data
from sensors on the subject's body. This report attempts to use the sensor
data to identify which classe each repetition falls under (correctly 
performed, or one of 4 common mistakes).

Of the methods tested, Random Forests was found to perform the best, though
boosted trees follows closely. By adjusting the Random Forest parameter to
use 4 random variables per tree, we were able to correctly predict all 20 of
the test cases.

Further improvement may be possible by increasing the tree count and by 
investigating (and eliminating, if warranted) some outlier values in the
training dataset that could represent errors in data collection.

```{r get data}
data_src_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn"
train_filename <- "pml-training.csv"
test_filename <- "pml-testing.csv"
data_dir <- "."
data_dir_files <- list.files(data_dir)

if (!train_filename %in% data_dir_files | !test_filename %in% data_dir_files) {
    download.file(paste(data_src_url, train_filename, 
                        sep = "/", dest = "data_dir"))
    download.file(paste(data_src_url, test_filename, 
                        sep = "/", dest = "data_dir"))
}
```

```{r load data}
df.train <- read.csv(paste(data_dir,train_filename, sep = "/"))
df.test <- read.csv(paste(data_dir,test_filename, sep = "/"))
```

# Explore the data
```{r, echo=TRUE}
dim(df.train)
```
```{r summary.all, eval=FALSE, echo=TRUE}
summary(df.train)
```

See the appendix for the output of the summary call, but
it has 19622 rows, with plenty of columns not populating
them very well.
There seems to be a pattern of columns missing 19216 items, 
and several have `#DIV/0` errors as well. 

This is enough to make the column useless, so we will
just get rid of them.

```{r, eval=FALSE}
# So far, I haven't seen 
# any columns with errors without the blanks, but some have blanks
# but no errors. The `new_window` variable has 19216 `no` values.
# Coincidence?
df.train %>% filter(new_window == "no") %>% summary()
# Not a coincidence, but not actually helpful as far as I can tell.
```

What about variable classes?
```{r classes.all, echo=TRUE, eval=FALSE}
str(df.train)
```

Again, output is in the appendix.
What we learn is that everything with an error has become 
a factor variable, which may complicate filtering them a bit.
Luckily, it looks like every
column with errors also has the 19216 blank values so they will
get filtered that way.

We will remove every variable that has more than 2000 errors or
missing values, knowing that most or all of these have over
20000.

Also drop a few columns that serve as de facto row identifiers
and will mess with results.

```{r choose variables, echo = TRUE}
df.train <- df.train %>% 
    # 2000 chosen arbitrarily as ~10% of variables
    select_if(~sum(is.na(.)) < 2000 & sum(. == "") < 2000) %>%
    # Remove the variables that allow identifying rows
    select(-X, -contains("timestamp"), -new_window, -num_window)

# dim(df.train)
```

```{r, eval=FALSE}
# These are arbitrarily selected and it's a waste of time.
qplot(classe, y = magnet_forearm_x, data = df.train)
qplot(classe, y = accel_dumbbell_x, data = df.train)
```


```{r boxplots.all, eval=FALSE, echo=FALSE}
df.train %>% 
    select(-user_name) %>%
    gather(key = key, value = value, -classe) %>%
    ggplot(aes(classe, value)) + geom_boxplot() +
    facet_wrap(~ key, scales = "free_y", ncol=5)

```

A pairplot to look for relationships (in the Appendix) shows that 
Classe A seems to have some important outliers:

 * There's a low one in `gyros_dumbbell_x` and a high one in each
 of `gyros_dumbbell_y` and `gyros_dumbbell_z`
 * The same pattern shows up in `gyros_forearm...`
 * There's a less extreme low outlier in `magnet_dumbbell_y`
 
 
```{r, eval=FALSE}
filter(df.train, gyros_dumbbell_x < -100)
```

This isn't a dealbreaker right now, but it's worth remembering
in case later results are strange.

# Feature Reduction
Running through some of the models the first time puts a strain
on my computer, to the point that it doesn't always finish
without running out of memory. Here are a couple of options to
reduce.

## findCorrelation

`findCorrelation` returns a list of columns to remove to reduce
correlation. A lower cutoff value will be more aggressive.

```{r, echo=TRUE}
cutoff <- 0.75
cor(select(df.train, -classe, -user_name)) %>%
    findCorrelation(cutoff=cutoff, names=T)
```

## varImp by Tree model

Strictly speaking model building comes later, but use a tree
model for variable importance.

```{r, echo = TRUE}
set.seed(1984)
tree <- train(classe ~ ., data = df.train, method = "rpart")
imp <- varImp(tree)$importance 
imp$var <- row.names(imp) 
imp <- imp %>% 
    filter(Overall > 0) %>%
    arrange(-Overall)

feature.cols <- imp$var
feature.cols
```

There is some overlap between the methods. We will use the 
model-based version since I expect it to be better at identfying
complex relationships than the correlation model.

# Models

We will used cross-validation to compare the models. 10 folds
should be good to strike a balance between bias reduction and
not straining my computer (or patience).

```{r models, cache=TRUE, include=FALSE, echo=TRUE}
# Go get out for coffee or something, this will take a while.
control <- trainControl(method="cv", 
                        number=10)

df.train.filtered <- select(df.train, feature.cols, classe)
tree <- train(x = select(df.train.filtered, -classe), 
              y = df.train.filtered$classe,
              method = "rpart", trControl = control)
lda <- train(x = select(df.train.filtered, -classe), 
              y = df.train.filtered$classe,
              method = "lda", trControl = control)

rf <- train(x = select(df.train.filtered, -classe), 
              y = df.train.filtered$classe,
              method = "rf", trControl = control)
gbm <- train(x = select(df.train.filtered, -classe), 
              y = df.train.filtered$classe,
              method = "gbm", trControl = control)
```

Now we can compare these methods by confusion matrices.

### Decision Tree
```{r}
confusionMatrix.train(tree)
```

### Random Forest
```{r}
confusionMatrix.train(rf)
```

### Gradient Boosting Machine (boosted tree)
```{r}
confusionMatrix.train(gbm)
```

### Linear Discriminant Analysis
```{r}
confusionMatrix.train(lda)
```

Random forest is the clear best bet, but can we improve it?

```{r}
plot(rf)
```

```{r}
rf.err <- as_tibble(rf$finalModel$err.rate) %>%
    mutate(ntree = 1:500)

g <- ggplot(rf.err, aes(x = ntree, y = OOB))
g <- g + geom_point()
g <- g + theme_bw()
g
```

The out-of-bag error seems to be mostly stabilized around 100
trees, but does still continue to improve slightly. 
We will leave it as-is, and increase later if we need it.

So 2 is the best of our tried `mtry` values (2, 8, 14), and 
there is maybe a trend of improvement as they decrease. We will
test 1 and a few in the high end range between 2 and 8.
 
# Final Model
```{r second rf, cache=TRUE, echo=TRUE}
grid <- expand.grid(mtry = c(1, 2, 4, 6))

rf2 <- train(x = select(df.train.filtered, -classe),
             y = df.train.filtered$classe,
             method = "rf", trControl = control, 
             tuneGrid = grid)

```

```{r}
plot(rf2)
rf2
```

So our preferred `mtry` value is 4, which gives an accuracy of 
99.19% and an out of sample error rate of
`r rf2$finalModel$err.rate[500, 'OOB']`, which turns out to be
good enough for the quiz.

# Appendix
### Long outputs

```{r summary.all, echo=TRUE, eval=TRUE}
```
```{r classes.all, echo=TRUE, eval=TRUE}
```
```{r boxplots.all, echo=TRUE, eval=TRUE,fig.dim = c(9,15)}
```


